Trigger and DAQ (TDAQ)
----------------

Proposal for 2011 menu
https://cdsweb.cern.ch/record/1324514/files/ATL-COM-DAQ-2011-007.pdf

- Each trigger level refines decisions made at previous level and adds additional selection criteria
- DAQ receives and buffers the event data from the detector-specific readout electronics, at the L1 trigger accept rate, over 1600 point-to-point readout links

- EF output up to 200 Hz with event size of 1.3-1.5 Mbyte (according to detector paper - is this higher now? Delayed stream?)
   - average recording rate of about 300-400 Hz according to ICHEP proc

-  select events by identifying muons, electrons, photons, taus, jets and B hadron candidates, as well as using global event signatures, such as missing transverse energy.

- During 2011 and 2012 trigger needed constant adaptation to deal with increasing lumi and lessons learnt.
   - increase in the number of collisions per beam crossing (pileup) from approximately 2 in 2010, to 17 in 2011 and more than 35 in 2012, exceeding the original design value of 23 pileup events, provides a strong challenge to the trigger and necessitated multiple changes during this period 
   
- offline computing processor power limits rate to 400 Hz

- Critical importance as without trigger cannot do analysis, or will have suboptimal sensitivity.
- Balance between sensitivity and rates/CPU consumption within limits.
- Analysis preference for stable trigger performance and efficiency.
   
L1
===
- L1 uses  custom electronics with input from the calorimeter and muon detectors,
   - L1 deciison time 2.5 us, rate to 75 kHz ( in 2010 with all detector systems was 50 kHz, but software upgrades of
the limiting systems were done before 2011. Was higher rate achieved?)
   - fast, custom electronics using low-granularity signals from the calorimeters and fast signals from dedicated muon trigger chambers.
   - Requires prescence of signals from hard scattering such as high pT electron, muon, jet or MET
   - Also identifies RoIs around high pT objects for later trigger levels.
   
L2 & EF
=======
- L2 and EF software based
   - About 8000 cores for each level
   - L2 full event not availavble, but request data for RoI identified by L1. Uses dedicated fast algorithms (<100ms vg).
   - EF has full event data and up to a second, though generally uses only RoI data
   - EF algorithms based on offline algo, adapted for trigger to give best performance.

Organisation
==============
- Organied into chains, consisting of a L1 selection seeding a chain of algorithms in HLT. 
- Each chain responsible for selection specific trigger signature
- trigger menu ctypically consists of about 500 chains- about 200 primary physics and 300 supporting for bg and efficiencies.

Triggers in 2011 / 2012
===========================
- Two distinct menus in 2011, one for 1-2 x10^33, one for 3-5x10^33. 
- 2012 menu redesigned to compe with up to 8x10^33, very high pileup
- BAndwidth split between different phsycis signatures depending on importance / how refined trigger is.
- Single el and muon triggers are most refined and take highest share of bandwidth - 50 Hz or more each.
- Other multipurpose triggers (e.g. multijets) typically take 5-15 Hz and analsysi spesicif triggers have ~1Hz each.
- In 2012, delayed streaming - record additional 150 Hz of loer pT B-physics and jet triggers for reco in 2013.
- In 2011 The L1 output rate was kept below 60 kHz, L2 output rate below 5 kHz and EF output rate at about 400 Hz averaged over the LHC fills. 
 - Above 65 kHz dead time from detectors observed.
 - L2 limis comes from detector ROS maximum ate (6 kHz) and CPU (7 kHz). EF limit driven by tier0 computing
 - Hard limit from TDAQ is about 600 Hz.

Muon Triggers
=============
- Not affected significantly by pileup, but increased lumi requires tightening of pT selection in 2012.
   - 2011: 18 GeV threshold
   - 2012: 24 GeV, plus isolation: ptcone20/pt<12%. Tracks have to have z0<6mm
   
Electron triggers
==================

2011: The bandwidth allocated to the electron and photon triggers was about 30% of the total EF bandwidth.

L1 vh?

- Higher fake rates than for muons.
- Had to be tightened suring 2011 running to control rates whilst keeping good acceptance
- Mid 2011, tighten both L1 and HLT. 2012, reoptimise ID to remove pu dependance and add ID

- L1 granularity detaxdpho 0.1x0.1 (trigger towers) to identify pos of RoI and compute ET of clusters precision 1 GeV
- for each tower, sum cells of EM or had cal, except for foruth layer of had endcap and barrel endcap gap scintallators
- EM clusters formed by identifying local maximas using sliding window algorithm on 4x4 trigger towers. 
- Trigger satisfied if the window's core region, (2x2) contains a pair of neighbouring towers witrh combined energy over threshold.

- Calorimeter cells responses are electric current pulses
- Recieved and chaped in FE electronics of teh detectors
- FE electronics digitise  when the accpet signal is recieved from L1
- Need to process 5 (7) 25ns samples from lar (tile) in order to precisely measure pulse height when strong contribution from events in other crossings - optimal filtering.
Since there are almost 200e3 calo cells, need an array of parallel units - 700 Digital Signal Processors (DSPs). Rate is upt o 100 kHz.
- Constants used in DSP cannot be changed during run, but conditions (eg Lar HV) can chane, which varies signal by up to 10 or 15%. Offline reprocessing of L2/EF may then give different result.

- L2: Build clustsers sithin 0.4x0.4 RoI identified at L1.
   - Use only second layer of EM cal to find the cell with the largest Et in the RoI - pre-seed.
   - Find position by taking energy weighted average of 3x7 cells surrounding pre-seed.
   - Find energy in 3x7 (0.075x0.175) grid in barrel (<1.4) or 5x5 (.125x.125) in EC (1.4<2.47)
   - Apply corrections based on offline algorithms to improve resoltuion of pos and energy.
   - RÎ·,Eratio shower shape variable cuts

- EF: use offline like algorithms
   - take cell info from slightly larger than RoI
   - use offline sliding window algo to build cluster and apply offline corrections.
   
- Changes in 2011 to keep rates low:
   - Decrease gaps between L1 and HLT thresholds ("T" ttriggers)
   - variable thresholds depending on dead material ("v")
   - hadronic leakage requirement ("h")
   - tighten L2 cuts, increasingly close to EF requirements
   - EF threshold raised 20 to 22 when luminosity exceeded 2e33.
   - ID requirements tighetened in Sept for ze33 (medium->medium1).
   - medium1 designed to correspond to medium++